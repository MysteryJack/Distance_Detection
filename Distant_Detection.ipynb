{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distant_Detection",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1C7hAKj9S4mPEjnkoHCmcg3dFx2Y0SWRG",
      "authorship_tag": "ABX9TyNtVihjFylKkzCYlLmskqS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MysteryJack/Distance_Detection/blob/main/Distant_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPWWED9_aqrS"
      },
      "source": [
        "!pip install numpy\n",
        "!pip install tensorflow\n",
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVeop2oMaCc0"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49LmF9nUDUG"
      },
      "source": [
        "class DetectorAPI:\n",
        "    def __init__(self, path_to_ckpt):\n",
        "        self.path_to_ckpt = path_to_ckpt\n",
        "\n",
        "        self.detection_graph = tf.Graph()\n",
        "        with self.detection_graph.as_default():\n",
        "            od_graph_def = tf.compat.v1.GraphDef()\n",
        "            with tf.compat.v2.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n",
        "                serialized_graph = fid.read()\n",
        "                od_graph_def.ParseFromString(serialized_graph)\n",
        "                tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "        self.default_graph = self.detection_graph.as_default()\n",
        "        self.sess = tf.compat.v1.Session(graph=self.detection_graph)\n",
        "        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "    def processFrame(self, image):\n",
        "        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n",
        "        image_np_expanded = np.expand_dims(image, axis=0)\n",
        "        #Detection\n",
        "        start_time = time.time()\n",
        "        (boxes, scores, classes, num) = self.sess.run(\n",
        "            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
        "            feed_dict={self.image_tensor: image_np_expanded})\n",
        "        end_time = time.time()\n",
        "\n",
        "        print(\"Elapsed Time:\", end_time-start_time)\n",
        "        im_height, im_width,_ = image.shape\n",
        "        boxes_list = [None for i in range(boxes.shape[1])]\n",
        "        for i in range(boxes.shape[1]):\n",
        "            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
        "                        int(boxes[0,i,1]*im_width),\n",
        "                        int(boxes[0,i,2] * im_height),\n",
        "                        int(boxes[0,i,3]*im_width))\n",
        "\n",
        "        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
        "\n",
        "    def close(self):\n",
        "        self.sess.close()\n",
        "        self.default_graph.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "ffyKfie5U0_O",
        "outputId": "21581dce-f27f-45b3-e746-313ac498cfe5"
      },
      "source": [
        "model_path = r\"/content/drive/MyDrive/coding file for colab/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\"\n",
        "odapi = DetectorAPI(path_to_ckpt=model_path)\n",
        "threshold = 0.8\n",
        "line_op = 0.5\n",
        "cap = cv2.VideoCapture(r'/content/drive/MyDrive/coding file for colab/input/video.avi')\n",
        "img_array = []\n",
        "start_time = time.time()\n",
        "\n",
        "while True:\n",
        "    r, img = cap.read()\n",
        "    if r:  \n",
        "      size = (1280,720) \n",
        "      try:\n",
        "        img = cv2.resize(img, size)\n",
        "      except:\n",
        "        print(\"Error Resizing\")\n",
        "      line_layer = np.zeros(img.shape, np.uint8)\n",
        "      box_layer = np.zeros(img.shape, np.uint8)\n",
        "      boxes, scores, classes, num = odapi.processFrame(img)\n",
        "      center = []\n",
        "      distance = []\n",
        "        \n",
        "      # Visualization of the results of a detection.\n",
        "      for i in range(len(boxes)):\n",
        "          # Class 1 represents human\n",
        "          if classes[i] == 1 and scores[i] > threshold:\n",
        "            box = boxes[i]\n",
        "            b_center = (int(box[3]/2+box[1]/2), int(box[2]/2+box[0]/2))\n",
        "            center.append(b_center)\n",
        "            cv2.rectangle(box_layer,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)\n",
        "            cv2.putText(box_layer, 'Person ' + str(len(center)), (box[1],box[0]), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)\n",
        "      nbox = len(center)\n",
        "    \n",
        "      for n in range(nbox) :\n",
        "          for m in [i + (n + 1) for i in range((nbox-1)-n)]:\n",
        "              cv2.line(line_layer, center[n], center[m], (255,255,255),2)\n",
        "        \n",
        "      o1_layer = cv2.addWeighted(img, 1.0, line_layer, line_op, 0)\n",
        "      o2_layer = cv2.addWeighted(o1_layer, 1.0, box_layer, 1.0, 0)\n",
        "        \n",
        "\n",
        "      img_array.append(o2_layer)\n",
        "      out = cv2.VideoWriter('o_video.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        "    \n",
        "      for i in range(len(img_array)):\n",
        "          out.write(img_array[i])\n",
        "      \n",
        "    else:\n",
        "      end_time = time.time()\n",
        "      out.release()\n",
        "      print(\"Total Elapsed Time:\", end_time-start_time)\n",
        "      break\n",
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed Time: 0.08936595916748047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2127add2b337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m           \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}